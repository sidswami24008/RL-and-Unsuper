import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from collections import deque
import random
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer


# Set the seed for reproducibility
np.random.seed(0)
tf.random.set_seed(0)


class WordGenerator:
    def __init__(self):
        self.words = set()
        self.definitions = {}

    def generate_word(self):
        while True:
            word = ''.join(random.choice('abcdefghijklmnopqrstuvwxyz') for _ in range(5))
            if word not in self.words:
                self.words.add(word)
                return word

    def generate_definition(self, word):
        synsets = wordnet.synsets(word)
        if synsets:
            definition = synsets[0].definition()
        else:
            definition = "Definition not available"
        self.definitions[word] = definition
        return definition

    def save_word_library(self, path):
        with open(path, 'w') as file:
            for word in self.words:
                file.write(word + '\n')

    def save_definition_dictionary(self, path):
        with open(path, 'w') as file:
            for word, definition in self.definitions.items():
                file.write(f"{word}: {definition}\n")


class QNetwork:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=10000)
        self.gamma = 0.95  # Discount factor
        self.epsilon = 1.0  # Exploration rate
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        self.learning_rate = 0.001
        self.model = self.build_model()

    def build_model(self):
        input_layer = keras.layers.Input(shape=(self.state_size,))
        hidden_layers = keras.layers.Dense(128, activation='relu')(input_layer)
        lstm_layer = keras.layers.Bidirectional(keras.layers.GRU(128, return_sequences=True))(hidden_layers)
        output_layer = keras.layers.Dense(self.action_size, activation='linear')(lstm_layer)
        model = keras.models.Model(inputs=input_layer, outputs=output_layer)
        model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate))
        return model

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        q_values = self.model.predict(state)
        return np.argmax(q_values[0])

    def replay(self, batch_size):
        if len(self.memory) < batch_size:
            return
        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))
            target_f = self.model.predict(state)
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)

    def update_epsilon(self):
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def save_weights(self, path):
        self.model.save_weights(path)

    def load_weights(self, path):
        self.model.load_weights(path)


def train_model(num_episodes, batch_size):
    state_size = len(word_generator.words)
    action_size = len(word_generator.words)
    q_network = QNetwork(state_size, action_size)
    episode_accuracy = []

    for episode in range(1, num_episodes + 1):
        word = random.choice(list(word_generator.words))
        definition = word_generator.generate_definition(word)
        state = np.zeros((1, state_size))
        index = list(word_generator.words).index(word)
        state[0][index] = 1
        total_reward = 0
        done = False

        while not done:
            action = q_network.act(state)
            next_state = np.zeros((1, state_size))
            next_state[0][action] = 1

            # Check if the action word is the generated word
            if list(word_generator.words)[action] == word:
                reward = 1  # Word generation is a reward
            else:
                reward = 0

            total_reward += reward
            q_network.remember(state, action, reward, next_state, done)
            state = next_state

        q_network.replay(batch_size)
        q_network.update_epsilon()

        episode_accuracy.append(total_reward)

        print(f"Episode: {episode}/{num_episodes} Accuracy: {total_reward / len(word) * 100:.2f}%")
        print(f"Word: {word}, Definition: {definition}")
        print()

    print("Training complete.\n")

    # Display learned words and their meanings
    print("Learned Words and Meanings:")
    for word in word_generator.words:
        print(f"Word: {word}, Definition: {word_generator.definitions.get(word, 'Definition not available')}")
    print()

    # Save the learned words and definitions
    word_generator.save_word_library("learned_words.txt")
    word_generator.save_definition_dictionary("learned_definitions.txt")

    # Save the Q-network weights
    q_network.save_weights("q_network_weights.h5")

    # Plot episode accuracy
    plt.plot(episode_accuracy)
    plt.xlabel("Episode")
    plt.ylabel("Accuracy")
    plt.title("Episode Accuracy")
    plt.show()


word_generator = WordGenerator()
train_model(num_episodes=1000, batch_size=32)
